{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Package Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-24T03:52:09.739712Z",
     "start_time": "2019-12-24T03:52:09.736901Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0,'/Users/ankur.kumar/Desktop/Personal/Projects/open source projects/ALBERT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-24T04:03:22.102690Z",
     "start_time": "2019-12-24T04:03:22.092956Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import collections\n",
    "import random\n",
    "import tokenization\n",
    "import numpy as np\n",
    "import six\n",
    "from six.moves import range\n",
    "from six.moves import zip\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-24T04:10:05.894927Z",
     "start_time": "2019-12-24T04:10:05.885659Z"
    }
   },
   "outputs": [],
   "source": [
    "flags = tf.flags\n",
    "\n",
    "FLAGS = flags.FLAGS\n",
    "\n",
    "flags.DEFINE_string(\"input_file\", 'data/sample.txt',\n",
    "                    \"Input raw text file (or comma-separated list of files).\")\n",
    "\n",
    "flags.DEFINE_string(\n",
    "    \"output_file\", \"data/processed/sample\",\n",
    "    \"Output TF example file (or comma-separated list of files).\")\n",
    "\n",
    "flags.DEFINE_string(\n",
    "    \"vocab_file\", '../models/assets/30k-clean.vocab',\n",
    "    \"The vocabulary file that the ALBERT model was trained on.\")\n",
    "\n",
    "flags.DEFINE_string(\"spm_model_file\", \"../models/assets/30k-clean.model\",\n",
    "                    \"The model file for sentence piece tokenization.\")\n",
    "\n",
    "flags.DEFINE_bool(\n",
    "    \"do_lower_case\", True,\n",
    "    \"Whether to lower case the input text. Should be True for uncased \"\n",
    "    \"models and False for cased models.\")\n",
    "\n",
    "flags.DEFINE_bool(\n",
    "    \"do_whole_word_mask\", True,\n",
    "    \"Whether to use whole word masking rather than per-WordPiece masking.\")\n",
    "\n",
    "flags.DEFINE_bool(\n",
    "    \"do_permutation\", False,\n",
    "    \"Whether to do the permutation training.\")\n",
    "\n",
    "flags.DEFINE_bool(\n",
    "    \"favor_shorter_ngram\", False,\n",
    "    \"Whether to set higher probabilities for sampling shorter ngrams.\")\n",
    "\n",
    "flags.DEFINE_bool(\n",
    "    \"random_next_sentence\", False,\n",
    "    \"Whether to use the sentence that's right before the current sentence \"\n",
    "    \"as the negative sample for next sentence prection, rather than using \"\n",
    "    \"sentences from other random documents.\")\n",
    "\n",
    "flags.DEFINE_integer(\"max_seq_length\", 512, \"Maximum sequence length.\")\n",
    "\n",
    "flags.DEFINE_integer(\"ngram\", 3, \"Maximum number of ngrams to mask.\")\n",
    "\n",
    "flags.DEFINE_integer(\"max_predictions_per_seq\", 20,\n",
    "                     \"Maximum number of masked LM predictions per sequence.\")\n",
    "\n",
    "flags.DEFINE_integer(\"random_seed\", 12345, \"Random seed for data generation.\")\n",
    "\n",
    "flags.DEFINE_integer(\n",
    "    \"dupe_factor\", 40,\n",
    "    \"Number of times to duplicate the input data (with different masks).\")\n",
    "\n",
    "flags.DEFINE_float(\"masked_lm_prob\", 0.15, \"Masked LM probability.\")\n",
    "\n",
    "flags.DEFINE_float(\n",
    "    \"short_seq_prob\", 0.1,\n",
    "    \"Probability of creating sequences which are shorter than the \"\n",
    "    \"maximum length.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-24T04:15:00.524493Z",
     "start_time": "2019-12-24T04:15:00.521563Z"
    }
   },
   "outputs": [],
   "source": [
    "flags.DEFINE_string('f', '', 'kernel')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-24T04:25:06.381262Z",
     "start_time": "2019-12-24T04:25:06.298685Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class TrainingInstance(object):\n",
    "  \"\"\"A single training instance (sentence pair).\"\"\"\n",
    "\n",
    "  def __init__(self, tokens, segment_ids, masked_lm_positions, masked_lm_labels,\n",
    "               is_random_next, token_boundary):\n",
    "    self.tokens = tokens\n",
    "    self.segment_ids = segment_ids\n",
    "    self.is_random_next = is_random_next\n",
    "    self.token_boundary = token_boundary\n",
    "    self.masked_lm_positions = masked_lm_positions\n",
    "    self.masked_lm_labels = masked_lm_labels\n",
    "\n",
    "  def __str__(self):\n",
    "    s = \"\"\n",
    "    s += \"tokens: %s\\n\" % (\" \".join(\n",
    "        [tokenization.printable_text(x) for x in self.tokens]))\n",
    "    s += \"segment_ids: %s\\n\" % (\" \".join([str(x) for x in self.segment_ids]))\n",
    "    s += \"token_boundary: %s\\n\" % (\" \".join(\n",
    "        [str(x) for x in self.token_boundary]))\n",
    "    s += \"is_random_next: %s\\n\" % self.is_random_next\n",
    "    s += \"masked_lm_positions: %s\\n\" % (\" \".join(\n",
    "        [str(x) for x in self.masked_lm_positions]))\n",
    "    s += \"masked_lm_labels: %s\\n\" % (\" \".join(\n",
    "        [tokenization.printable_text(x) for x in self.masked_lm_labels]))\n",
    "    s += \"\\n\"\n",
    "    return s\n",
    "\n",
    "  def __repr__(self):\n",
    "    return self.__str__()\n",
    "\n",
    "\n",
    "def write_instance_to_example_files(instances, tokenizer, max_seq_length,\n",
    "                                    max_predictions_per_seq, output_files):\n",
    "  \"\"\"Create TF example files from `TrainingInstance`s.\"\"\"\n",
    "  writers = []\n",
    "  for output_file in output_files:\n",
    "    writers.append(tf.python_io.TFRecordWriter(output_file))\n",
    "\n",
    "  writer_index = 0\n",
    "\n",
    "  total_written = 0\n",
    "  for (inst_index, instance) in enumerate(instances):\n",
    "    input_ids = tokenizer.convert_tokens_to_ids(instance.tokens)\n",
    "    input_mask = [1] * len(input_ids)\n",
    "    segment_ids = list(instance.segment_ids)\n",
    "    token_boundary = list(instance.token_boundary)\n",
    "    assert len(input_ids) <= max_seq_length\n",
    "\n",
    "    while len(input_ids) < max_seq_length:\n",
    "      input_ids.append(0)\n",
    "      input_mask.append(0)\n",
    "      segment_ids.append(0)\n",
    "      token_boundary.append(0)\n",
    "\n",
    "    assert len(input_ids) == max_seq_length\n",
    "    assert len(input_mask) == max_seq_length\n",
    "    assert len(segment_ids) == max_seq_length\n",
    "\n",
    "    masked_lm_positions = list(instance.masked_lm_positions)\n",
    "    masked_lm_ids = tokenizer.convert_tokens_to_ids(instance.masked_lm_labels)\n",
    "    masked_lm_weights = [1.0] * len(masked_lm_ids)\n",
    "\n",
    "    multiplier = 1 + int(FLAGS.do_permutation)\n",
    "    while len(masked_lm_positions) < max_predictions_per_seq * multiplier:\n",
    "      masked_lm_positions.append(0)\n",
    "      masked_lm_ids.append(0)\n",
    "      masked_lm_weights.append(0.0)\n",
    "\n",
    "    sentence_order_label = 1 if instance.is_random_next else 0\n",
    "\n",
    "    features = collections.OrderedDict()\n",
    "    features[\"input_ids\"] = create_int_feature(input_ids)\n",
    "    features[\"input_mask\"] = create_int_feature(input_mask)\n",
    "    features[\"segment_ids\"] = create_int_feature(segment_ids)\n",
    "    features[\"token_boundary\"] = create_int_feature(token_boundary)\n",
    "    features[\"masked_lm_positions\"] = create_int_feature(masked_lm_positions)\n",
    "    features[\"masked_lm_ids\"] = create_int_feature(masked_lm_ids)\n",
    "    features[\"masked_lm_weights\"] = create_float_feature(masked_lm_weights)\n",
    "    # Note: We keep this feature name `next_sentence_labels` to be compatible\n",
    "    # with the original data created by lanzhzh@. However, in the ALBERT case\n",
    "    # it does contain sentence_order_label.\n",
    "    features[\"next_sentence_labels\"] = create_int_feature(\n",
    "        [sentence_order_label])\n",
    "\n",
    "    tf_example = tf.train.Example(features=tf.train.Features(feature=features))\n",
    "\n",
    "    writers[writer_index].write(tf_example.SerializeToString())\n",
    "    writer_index = (writer_index + 1) % len(writers)\n",
    "\n",
    "    total_written += 1\n",
    "\n",
    "    if inst_index < 20:\n",
    "      tf.logging.info(\"*** Example ***\")\n",
    "      tf.logging.info(\"tokens: %s\" % \" \".join(\n",
    "          [tokenization.printable_text(x) for x in instance.tokens]))\n",
    "\n",
    "      for feature_name in features.keys():\n",
    "        feature = features[feature_name]\n",
    "        values = []\n",
    "        if feature.int64_list.value:\n",
    "          values = feature.int64_list.value\n",
    "        elif feature.float_list.value:\n",
    "          values = feature.float_list.value\n",
    "        tf.logging.info(\n",
    "            \"%s: %s\" % (feature_name, \" \".join([str(x) for x in values])))\n",
    "\n",
    "  for writer in writers:\n",
    "    writer.close()\n",
    "\n",
    "  tf.logging.info(\"Wrote %d total instances\", total_written)\n",
    "\n",
    "\n",
    "def create_int_feature(values):\n",
    "  feature = tf.train.Feature(int64_list=tf.train.Int64List(value=list(values)))\n",
    "  return feature\n",
    "\n",
    "\n",
    "def create_float_feature(values):\n",
    "  feature = tf.train.Feature(float_list=tf.train.FloatList(value=list(values)))\n",
    "  return feature\n",
    "\n",
    "\n",
    "def create_training_instances(input_files, tokenizer, max_seq_length,\n",
    "                              dupe_factor, short_seq_prob, masked_lm_prob,\n",
    "                              max_predictions_per_seq, rng):\n",
    "  \"\"\"Create `TrainingInstance`s from raw text.\"\"\"\n",
    "  all_documents = [[]]\n",
    "\n",
    "  # Input file format:\n",
    "  # (1) One sentence per line. These should ideally be actual sentences, not\n",
    "  # entire paragraphs or arbitrary spans of text. (Because we use the\n",
    "  # sentence boundaries for the \"next sentence prediction\" task).\n",
    "  # (2) Blank lines between documents. Document boundaries are needed so\n",
    "  # that the \"next sentence prediction\" task doesn't span between documents.\n",
    "  for input_file in input_files:\n",
    "    with tf.gfile.GFile(input_file, \"r\") as reader:\n",
    "      while True:\n",
    "        line = reader.readline()\n",
    "        if not FLAGS.spm_model_file:\n",
    "          line = tokenization.convert_to_unicode(line)\n",
    "        if not line:\n",
    "          break\n",
    "        if FLAGS.spm_model_file:\n",
    "          line = tokenization.preprocess_text(line, lower=FLAGS.do_lower_case)\n",
    "        else:\n",
    "          line = line.strip()\n",
    "\n",
    "        # Empty lines are used as document delimiters\n",
    "        if not line:\n",
    "          all_documents.append([])\n",
    "        tokens = tokenizer.tokenize(line)\n",
    "        if tokens:\n",
    "          all_documents[-1].append(tokens)\n",
    "\n",
    "  # Remove empty documents\n",
    "  all_documents = [x for x in all_documents if x]\n",
    "  rng.shuffle(all_documents)\n",
    "\n",
    "  vocab_words = list(tokenizer.vocab.keys())\n",
    "  instances = []\n",
    "  for _ in range(dupe_factor):\n",
    "    for document_index in range(len(all_documents)):\n",
    "      instances.extend(\n",
    "          create_instances_from_document(\n",
    "              all_documents, document_index, max_seq_length, short_seq_prob,\n",
    "              masked_lm_prob, max_predictions_per_seq, vocab_words, rng))\n",
    "\n",
    "  rng.shuffle(instances)\n",
    "  return instances\n",
    "\n",
    "\n",
    "def create_instances_from_document(\n",
    "    all_documents, document_index, max_seq_length, short_seq_prob,\n",
    "    masked_lm_prob, max_predictions_per_seq, vocab_words, rng):\n",
    "  \"\"\"Creates `TrainingInstance`s for a single document.\"\"\"\n",
    "  document = all_documents[document_index]\n",
    "\n",
    "  # Account for [CLS], [SEP], [SEP]\n",
    "  max_num_tokens = max_seq_length - 3\n",
    "\n",
    "  # We *usually* want to fill up the entire sequence since we are padding\n",
    "  # to `max_seq_length` anyways, so short sequences are generally wasted\n",
    "  # computation. However, we *sometimes*\n",
    "  # (i.e., short_seq_prob == 0.1 == 10% of the time) want to use shorter\n",
    "  # sequences to minimize the mismatch between pre-training and fine-tuning.\n",
    "  # The `target_seq_length` is just a rough target however, whereas\n",
    "  # `max_seq_length` is a hard limit.\n",
    "  target_seq_length = max_num_tokens\n",
    "  if rng.random() < short_seq_prob:\n",
    "    target_seq_length = rng.randint(2, max_num_tokens)\n",
    "\n",
    "  # We DON'T just concatenate all of the tokens from a document into a long\n",
    "  # sequence and choose an arbitrary split point because this would make the\n",
    "  # next sentence prediction task too easy. Instead, we split the input into\n",
    "  # segments \"A\" and \"B\" based on the actual \"sentences\" provided by the user\n",
    "  # input.\n",
    "  instances = []\n",
    "  current_chunk = []\n",
    "  current_length = 0\n",
    "  i = 0\n",
    "  while i < len(document):\n",
    "    segment = document[i]\n",
    "    current_chunk.append(segment)\n",
    "    current_length += len(segment)\n",
    "    if i == len(document) - 1 or current_length >= target_seq_length:\n",
    "      if current_chunk:\n",
    "        # `a_end` is how many segments from `current_chunk` go into the `A`\n",
    "        # (first) sentence.\n",
    "        a_end = 1\n",
    "        if len(current_chunk) >= 2:\n",
    "          a_end = rng.randint(1, len(current_chunk) - 1)\n",
    "\n",
    "        tokens_a = []\n",
    "        for j in range(a_end):\n",
    "          tokens_a.extend(current_chunk[j])\n",
    "\n",
    "        tokens_b = []\n",
    "        # Random next\n",
    "        is_random_next = False\n",
    "        if len(current_chunk) == 1 or \\\n",
    "            (FLAGS.random_next_sentence and rng.random() < 0.5):\n",
    "          is_random_next = True\n",
    "          target_b_length = target_seq_length - len(tokens_a)\n",
    "\n",
    "          # This should rarely go for more than one iteration for large\n",
    "          # corpora. However, just to be careful, we try to make sure that\n",
    "          # the random document is not the same as the document\n",
    "          # we're processing.\n",
    "          for _ in range(10):\n",
    "            random_document_index = rng.randint(0, len(all_documents) - 1)\n",
    "            if random_document_index != document_index:\n",
    "              break\n",
    "\n",
    "          random_document = all_documents[random_document_index]\n",
    "          random_start = rng.randint(0, len(random_document) - 1)\n",
    "          for j in range(random_start, len(random_document)):\n",
    "            tokens_b.extend(random_document[j])\n",
    "            if len(tokens_b) >= target_b_length:\n",
    "              break\n",
    "          # We didn't actually use these segments so we \"put them back\" so\n",
    "          # they don't go to waste.\n",
    "          num_unused_segments = len(current_chunk) - a_end\n",
    "          i -= num_unused_segments\n",
    "        elif not FLAGS.random_next_sentence and rng.random() < 0.5:\n",
    "          is_random_next = True\n",
    "          for j in range(a_end, len(current_chunk)):\n",
    "            tokens_b.extend(current_chunk[j])\n",
    "          # Note(mingdachen): in this case, we just swap tokens_a and tokens_b\n",
    "          tokens_a, tokens_b = tokens_b, tokens_a\n",
    "        # Actual next\n",
    "        else:\n",
    "          is_random_next = False\n",
    "          for j in range(a_end, len(current_chunk)):\n",
    "            tokens_b.extend(current_chunk[j])\n",
    "        truncate_seq_pair(tokens_a, tokens_b, max_num_tokens, rng)\n",
    "\n",
    "        assert len(tokens_a) >= 1\n",
    "        assert len(tokens_b) >= 1\n",
    "\n",
    "        tokens = []\n",
    "        segment_ids = []\n",
    "        tokens.append(\"[CLS]\")\n",
    "        segment_ids.append(0)\n",
    "        for token in tokens_a:\n",
    "          tokens.append(token)\n",
    "          segment_ids.append(0)\n",
    "\n",
    "        tokens.append(\"[SEP]\")\n",
    "        segment_ids.append(0)\n",
    "\n",
    "        for token in tokens_b:\n",
    "          tokens.append(token)\n",
    "          segment_ids.append(1)\n",
    "        tokens.append(\"[SEP]\")\n",
    "        segment_ids.append(1)\n",
    "\n",
    "        (tokens, masked_lm_positions,\n",
    "         masked_lm_labels, token_boundary) = create_masked_lm_predictions(\n",
    "             tokens, masked_lm_prob, max_predictions_per_seq, vocab_words, rng)\n",
    "        instance = TrainingInstance(\n",
    "            tokens=tokens,\n",
    "            segment_ids=segment_ids,\n",
    "            is_random_next=is_random_next,\n",
    "            token_boundary=token_boundary,\n",
    "            masked_lm_positions=masked_lm_positions,\n",
    "            masked_lm_labels=masked_lm_labels)\n",
    "        instances.append(instance)\n",
    "      current_chunk = []\n",
    "      current_length = 0\n",
    "    i += 1\n",
    "\n",
    "  return instances\n",
    "\n",
    "\n",
    "MaskedLmInstance = collections.namedtuple(\"MaskedLmInstance\",\n",
    "                                          [\"index\", \"label\"])\n",
    "\n",
    "\n",
    "def _is_start_piece_sp(piece):\n",
    "  \"\"\"Check if the current word piece is the starting piece (sentence piece).\"\"\"\n",
    "  special_pieces = set(list('!\"#$%&\\\"()*+,-./:;?@[\\\\]^_`{|}~'))\n",
    "  special_pieces.add(u\"€\".encode(\"utf-8\"))\n",
    "  special_pieces.add(u\"£\".encode(\"utf-8\"))\n",
    "  # Note(mingdachen):\n",
    "  # For foreign characters, we always treat them as a whole piece.\n",
    "  english_chars = set(list(\"abcdefghijklmnopqrstuvwxyz\"))\n",
    "  if (six.ensure_str(piece).startswith(\"▁\") or\n",
    "      six.ensure_str(piece).startswith(\"<\") or piece in special_pieces or\n",
    "      not all([i.lower() in english_chars.union(special_pieces)\n",
    "               for i in piece])):\n",
    "    return True\n",
    "  else:\n",
    "    return False\n",
    "\n",
    "\n",
    "def _is_start_piece_bert(piece):\n",
    "  \"\"\"Check if the current word piece is the starting piece (BERT).\"\"\"\n",
    "  # When a word has been split into\n",
    "  # WordPieces, the first token does not have any marker and any subsequence\n",
    "  # tokens are prefixed with ##. So whenever we see the ## token, we\n",
    "  # append it to the previous set of word indexes.\n",
    "  return not six.ensure_str(piece).startswith(\"##\")\n",
    "\n",
    "\n",
    "def is_start_piece(piece):\n",
    "  if FLAGS.spm_model_file:\n",
    "    return _is_start_piece_sp(piece)\n",
    "  else:\n",
    "    return _is_start_piece_bert(piece)\n",
    "\n",
    "\n",
    "def create_masked_lm_predictions(tokens, masked_lm_prob,\n",
    "                                 max_predictions_per_seq, vocab_words, rng):\n",
    "  \"\"\"Creates the predictions for the masked LM objective.\"\"\"\n",
    "\n",
    "  cand_indexes = []\n",
    "  # Note(mingdachen): We create a list for recording if the piece is\n",
    "  # the starting piece of current token, where 1 means true, so that\n",
    "  # on-the-fly whole word masking is possible.\n",
    "  token_boundary = [0] * len(tokens)\n",
    "\n",
    "  for (i, token) in enumerate(tokens):\n",
    "    if token == \"[CLS]\" or token == \"[SEP]\":\n",
    "      token_boundary[i] = 1\n",
    "      continue\n",
    "    # Whole Word Masking means that if we mask all of the wordpieces\n",
    "    # corresponding to an original word.\n",
    "    #\n",
    "    # Note that Whole Word Masking does *not* change the training code\n",
    "    # at all -- we still predict each WordPiece independently, softmaxed\n",
    "    # over the entire vocabulary.\n",
    "    if (FLAGS.do_whole_word_mask and len(cand_indexes) >= 1 and\n",
    "        not is_start_piece(token)):\n",
    "      cand_indexes[-1].append(i)\n",
    "    else:\n",
    "      cand_indexes.append([i])\n",
    "      if is_start_piece(token):\n",
    "        token_boundary[i] = 1\n",
    "\n",
    "  output_tokens = list(tokens)\n",
    "\n",
    "  masked_lm_positions = []\n",
    "  masked_lm_labels = []\n",
    "\n",
    "  if masked_lm_prob == 0:\n",
    "    return (output_tokens, masked_lm_positions,\n",
    "            masked_lm_labels, token_boundary)\n",
    "\n",
    "  num_to_predict = min(max_predictions_per_seq,\n",
    "                       max(1, int(round(len(tokens) * masked_lm_prob))))\n",
    "\n",
    "  # Note(mingdachen):\n",
    "  # By default, we set the probilities to favor longer ngram sequences.\n",
    "  ngrams = np.arange(1, FLAGS.ngram + 1, dtype=np.int64)\n",
    "  pvals = 1. / np.arange(1, FLAGS.ngram + 1)\n",
    "  pvals /= pvals.sum(keepdims=True)\n",
    "\n",
    "  if FLAGS.favor_shorter_ngram:\n",
    "    pvals = pvals[::-1]\n",
    "\n",
    "  ngram_indexes = []\n",
    "  for idx in range(len(cand_indexes)):\n",
    "    ngram_index = []\n",
    "    for n in ngrams:\n",
    "      ngram_index.append(cand_indexes[idx:idx+n])\n",
    "    ngram_indexes.append(ngram_index)\n",
    "\n",
    "  rng.shuffle(ngram_indexes)\n",
    "\n",
    "  masked_lms = []\n",
    "  covered_indexes = set()\n",
    "  for cand_index_set in ngram_indexes:\n",
    "    if len(masked_lms) >= num_to_predict:\n",
    "      break\n",
    "    if not cand_index_set:\n",
    "      continue\n",
    "    # Note(mingdachen):\n",
    "    # Skip current piece if they are covered in lm masking or previous ngrams.\n",
    "    for index_set in cand_index_set[0]:\n",
    "      for index in index_set:\n",
    "        if index in covered_indexes:\n",
    "          continue\n",
    "\n",
    "    n = np.random.choice(ngrams[:len(cand_index_set)],\n",
    "                         p=pvals[:len(cand_index_set)] /\n",
    "                         pvals[:len(cand_index_set)].sum(keepdims=True))\n",
    "    index_set = sum(cand_index_set[n - 1], [])\n",
    "    n -= 1\n",
    "    # Note(mingdachen):\n",
    "    # Repeatedly looking for a candidate that does not exceed the\n",
    "    # maximum number of predictions by trying shorter ngrams.\n",
    "    while len(masked_lms) + len(index_set) > num_to_predict:\n",
    "      if n == 0:\n",
    "        break\n",
    "      index_set = sum(cand_index_set[n - 1], [])\n",
    "      n -= 1\n",
    "    # If adding a whole-word mask would exceed the maximum number of\n",
    "    # predictions, then just skip this candidate.\n",
    "    if len(masked_lms) + len(index_set) > num_to_predict:\n",
    "      continue\n",
    "    is_any_index_covered = False\n",
    "    for index in index_set:\n",
    "      if index in covered_indexes:\n",
    "        is_any_index_covered = True\n",
    "        break\n",
    "    if is_any_index_covered:\n",
    "      continue\n",
    "    for index in index_set:\n",
    "      covered_indexes.add(index)\n",
    "\n",
    "      masked_token = None\n",
    "      # 80% of the time, replace with [MASK]\n",
    "      if rng.random() < 0.8:\n",
    "        masked_token = \"[MASK]\"\n",
    "      else:\n",
    "        # 10% of the time, keep original\n",
    "        if rng.random() < 0.5:\n",
    "          masked_token = tokens[index]\n",
    "        # 10% of the time, replace with random word\n",
    "        else:\n",
    "          masked_token = vocab_words[rng.randint(0, len(vocab_words) - 1)]\n",
    "\n",
    "      output_tokens[index] = masked_token\n",
    "\n",
    "      masked_lms.append(MaskedLmInstance(index=index, label=tokens[index]))\n",
    "  assert len(masked_lms) <= num_to_predict\n",
    "\n",
    "  rng.shuffle(ngram_indexes)\n",
    "\n",
    "  select_indexes = set()\n",
    "  if FLAGS.do_permutation:\n",
    "    for cand_index_set in ngram_indexes:\n",
    "      if len(select_indexes) >= num_to_predict:\n",
    "        break\n",
    "      if not cand_index_set:\n",
    "        continue\n",
    "      # Note(mingdachen):\n",
    "      # Skip current piece if they are covered in lm masking or previous ngrams.\n",
    "      for index_set in cand_index_set[0]:\n",
    "        for index in index_set:\n",
    "          if index in covered_indexes or index in select_indexes:\n",
    "            continue\n",
    "\n",
    "      n = np.random.choice(ngrams[:len(cand_index_set)],\n",
    "                           p=pvals[:len(cand_index_set)] /\n",
    "                           pvals[:len(cand_index_set)].sum(keepdims=True))\n",
    "      index_set = sum(cand_index_set[n - 1], [])\n",
    "      n -= 1\n",
    "\n",
    "      while len(select_indexes) + len(index_set) > num_to_predict:\n",
    "        if n == 0:\n",
    "          break\n",
    "        index_set = sum(cand_index_set[n - 1], [])\n",
    "        n -= 1\n",
    "      # If adding a whole-word mask would exceed the maximum number of\n",
    "      # predictions, then just skip this candidate.\n",
    "      if len(select_indexes) + len(index_set) > num_to_predict:\n",
    "        continue\n",
    "      is_any_index_covered = False\n",
    "      for index in index_set:\n",
    "        if index in covered_indexes or index in select_indexes:\n",
    "          is_any_index_covered = True\n",
    "          break\n",
    "      if is_any_index_covered:\n",
    "        continue\n",
    "      for index in index_set:\n",
    "        select_indexes.add(index)\n",
    "    assert len(select_indexes) <= num_to_predict\n",
    "\n",
    "    select_indexes = sorted(select_indexes)\n",
    "    permute_indexes = list(select_indexes)\n",
    "    rng.shuffle(permute_indexes)\n",
    "    orig_token = list(output_tokens)\n",
    "\n",
    "    for src_i, tgt_i in zip(select_indexes, permute_indexes):\n",
    "      output_tokens[src_i] = orig_token[tgt_i]\n",
    "      masked_lms.append(MaskedLmInstance(index=src_i, label=orig_token[src_i]))\n",
    "\n",
    "  masked_lms = sorted(masked_lms, key=lambda x: x.index)\n",
    "\n",
    "  for p in masked_lms:\n",
    "    masked_lm_positions.append(p.index)\n",
    "    masked_lm_labels.append(p.label)\n",
    "  return (output_tokens, masked_lm_positions, masked_lm_labels, token_boundary)\n",
    "\n",
    "\n",
    "def truncate_seq_pair(tokens_a, tokens_b, max_num_tokens, rng):\n",
    "  \"\"\"Truncates a pair of sequences to a maximum sequence length.\"\"\"\n",
    "  while True:\n",
    "    total_length = len(tokens_a) + len(tokens_b)\n",
    "    if total_length <= max_num_tokens:\n",
    "      break\n",
    "\n",
    "    trunc_tokens = tokens_a if len(tokens_a) > len(tokens_b) else tokens_b\n",
    "    assert len(trunc_tokens) >= 1\n",
    "\n",
    "    # We want to sometimes truncate from the front and sometimes from the\n",
    "    # back to add more randomness and avoid biases.\n",
    "    if rng.random() < 0.5:\n",
    "      del trunc_tokens[0]\n",
    "    else:\n",
    "      trunc_tokens.pop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## create_training_instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-24T04:44:21.919008Z",
     "start_time": "2019-12-24T04:44:21.915920Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "text = 'Our last article of short stories became so popular, that we decided to create another list, in which every story has a simple moral behind it.'\n",
    "line = tokenization.convert_to_unicode(text)\n",
    "line = tokenization.preprocess_text(line, lower=FLAGS.do_lower_case)\n",
    "line = line.strip()\n",
    "tokens = tokenizer.tokenize(line)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-24T04:44:26.956978Z",
     "start_time": "2019-12-24T04:44:26.953510Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁our',\n",
       " '▁last',\n",
       " '▁article',\n",
       " '▁of',\n",
       " '▁short',\n",
       " '▁stories',\n",
       " '▁became',\n",
       " '▁so',\n",
       " '▁popular',\n",
       " ',',\n",
       " '▁that',\n",
       " '▁we',\n",
       " '▁decided',\n",
       " '▁to',\n",
       " '▁create',\n",
       " '▁another',\n",
       " '▁list',\n",
       " ',',\n",
       " '▁in',\n",
       " '▁which',\n",
       " '▁every',\n",
       " '▁story',\n",
       " '▁has',\n",
       " '▁a',\n",
       " '▁simple',\n",
       " '▁moral',\n",
       " '▁behind',\n",
       " '▁it',\n",
       " '.']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-24T04:45:13.012617Z",
     "start_time": "2019-12-24T04:45:13.009468Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "vocab_words = list(tokenizer.vocab.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-24T04:45:15.958307Z",
     "start_time": "2019-12-24T04:45:15.944716Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<pad>',\n",
       " '<unk>',\n",
       " '[CLS]',\n",
       " '[SEP]',\n",
       " '[MASK]',\n",
       " '(',\n",
       " ')',\n",
       " '\"',\n",
       " '-',\n",
       " '.',\n",
       " '–',\n",
       " '£',\n",
       " '€',\n",
       " '▁',\n",
       " '▁the',\n",
       " ',',\n",
       " '▁of',\n",
       " '▁and',\n",
       " 's',\n",
       " '▁in',\n",
       " '▁to',\n",
       " '▁a',\n",
       " \"'\",\n",
       " '▁was',\n",
       " '▁he',\n",
       " '▁is',\n",
       " '▁for',\n",
       " '▁on',\n",
       " '▁as',\n",
       " '▁with',\n",
       " '▁that',\n",
       " '▁i',\n",
       " '▁it',\n",
       " '▁his',\n",
       " '▁by',\n",
       " '▁at',\n",
       " '▁her',\n",
       " '▁from',\n",
       " 't',\n",
       " '▁she',\n",
       " '▁an',\n",
       " '▁had',\n",
       " '▁you',\n",
       " 'd',\n",
       " '▁be',\n",
       " ':',\n",
       " '▁were',\n",
       " '▁but',\n",
       " '▁this',\n",
       " 'i',\n",
       " '▁are',\n",
       " '▁my',\n",
       " '▁not',\n",
       " '▁one',\n",
       " '▁or',\n",
       " '▁me',\n",
       " '▁which',\n",
       " '▁have',\n",
       " 'a',\n",
       " '▁they',\n",
       " '?',\n",
       " '▁him',\n",
       " 'e',\n",
       " '▁has',\n",
       " '▁first',\n",
       " '▁all',\n",
       " '▁their',\n",
       " '▁also',\n",
       " 'ing',\n",
       " 'ed',\n",
       " '▁out',\n",
       " '▁up',\n",
       " '▁who',\n",
       " ';',\n",
       " '▁been',\n",
       " '▁after',\n",
       " '▁when',\n",
       " '▁into',\n",
       " '▁new',\n",
       " 'm',\n",
       " '▁there',\n",
       " '▁two',\n",
       " '▁its',\n",
       " '▁would',\n",
       " '▁over',\n",
       " '▁time',\n",
       " '▁so',\n",
       " '▁said',\n",
       " '▁about',\n",
       " '▁other',\n",
       " '▁no',\n",
       " '▁more',\n",
       " '▁can',\n",
       " 'y',\n",
       " '▁then',\n",
       " '▁we',\n",
       " 'th',\n",
       " '▁back',\n",
       " '▁what',\n",
       " 're',\n",
       " '▁if',\n",
       " '▁like',\n",
       " 'ly',\n",
       " 'n',\n",
       " '▁only',\n",
       " '▁them',\n",
       " 'er',\n",
       " '▁do',\n",
       " 'in',\n",
       " '▁some',\n",
       " '▁could',\n",
       " 'o',\n",
       " '▁during',\n",
       " '▁where',\n",
       " '▁just',\n",
       " '▁before',\n",
       " '▁school',\n",
       " '▁made',\n",
       " '/',\n",
       " '▁than',\n",
       " '▁through',\n",
       " '▁de',\n",
       " '▁years',\n",
       " '▁may',\n",
       " 'the',\n",
       " '▁down',\n",
       " '▁world',\n",
       " '▁most',\n",
       " '▁between',\n",
       " '▁will',\n",
       " '▁now',\n",
       " '▁under',\n",
       " '▁three',\n",
       " '▁while',\n",
       " '▁well',\n",
       " '2',\n",
       " '▁city',\n",
       " '▁1',\n",
       " '▁later',\n",
       " 'r',\n",
       " '▁around',\n",
       " '▁part',\n",
       " '▁being',\n",
       " '▁know',\n",
       " '▁did',\n",
       " '▁such',\n",
       " '▁state',\n",
       " '▁used',\n",
       " '▁people',\n",
       " '▁against',\n",
       " 'c',\n",
       " '▁many',\n",
       " '▁national',\n",
       " '▁second',\n",
       " '▁your',\n",
       " '▁university',\n",
       " '▁both',\n",
       " '▁head',\n",
       " '▁these',\n",
       " '▁year',\n",
       " 'es',\n",
       " '▁way',\n",
       " '▁go',\n",
       " '▁until',\n",
       " '▁get',\n",
       " '1',\n",
       " '▁even',\n",
       " '▁known',\n",
       " '▁off',\n",
       " '▁man',\n",
       " '▁work',\n",
       " '▁film',\n",
       " '▁2',\n",
       " '▁team',\n",
       " '▁still',\n",
       " '▁long',\n",
       " '▁war',\n",
       " 'man',\n",
       " '▁became',\n",
       " '▁since',\n",
       " '▁south',\n",
       " '▁united',\n",
       " '▁us',\n",
       " '▁high',\n",
       " '▁how',\n",
       " '▁because',\n",
       " '▁any',\n",
       " '!',\n",
       " '▁again',\n",
       " '▁american',\n",
       " '▁family',\n",
       " '▁house',\n",
       " 'al',\n",
       " '▁right',\n",
       " '▁eyes',\n",
       " 've',\n",
       " '▁see',\n",
       " 'k',\n",
       " '▁season',\n",
       " '▁took',\n",
       " '▁north',\n",
       " '▁life',\n",
       " '▁states',\n",
       " '▁3',\n",
       " '▁name',\n",
       " '▁same',\n",
       " '▁each',\n",
       " '▁however',\n",
       " '▁day',\n",
       " '▁place',\n",
       " 'an',\n",
       " 'll',\n",
       " '▁much',\n",
       " '▁home',\n",
       " '▁group',\n",
       " '▁including',\n",
       " '▁found',\n",
       " '▁area',\n",
       " 'on',\n",
       " 'en',\n",
       " 'b',\n",
       " '▁don',\n",
       " '▁four',\n",
       " '▁didn',\n",
       " '▁hand',\n",
       " '▁left',\n",
       " '▁another',\n",
       " '▁called',\n",
       " '▁going',\n",
       " '▁away',\n",
       " '▁won',\n",
       " '▁series',\n",
       " '▁music',\n",
       " '▁make',\n",
       " '▁number',\n",
       " '▁here',\n",
       " '▁last',\n",
       " '▁company',\n",
       " '▁several',\n",
       " '▁john',\n",
       " '3',\n",
       " '▁end',\n",
       " 'it',\n",
       " '▁never',\n",
       " '▁album',\n",
       " 'you',\n",
       " '▁best',\n",
       " '▁take',\n",
       " 'or',\n",
       " '▁following',\n",
       " '▁game',\n",
       " 'no',\n",
       " 'h',\n",
       " '▁very',\n",
       " '▁good',\n",
       " 'l',\n",
       " '▁district',\n",
       " '▁played',\n",
       " '▁own',\n",
       " '▁want',\n",
       " '▁began',\n",
       " '▁released',\n",
       " 'to',\n",
       " 'g',\n",
       " '5',\n",
       " '▁little',\n",
       " '▁too',\n",
       " 'us',\n",
       " '▁4',\n",
       " '▁held',\n",
       " '▁side',\n",
       " '▁county',\n",
       " '▁c',\n",
       " '▁those',\n",
       " '▁early',\n",
       " '▁use',\n",
       " '▁face',\n",
       " '▁think',\n",
       " '▁league',\n",
       " '▁&',\n",
       " '▁west',\n",
       " '▁came',\n",
       " '▁air',\n",
       " '▁government',\n",
       " '▁small',\n",
       " '▁march',\n",
       " '▁town',\n",
       " '▁u',\n",
       " '▁club',\n",
       " '▁thought',\n",
       " 'and',\n",
       " 'u',\n",
       " '▁looked',\n",
       " '▁line',\n",
       " '▁international',\n",
       " '▁june',\n",
       " '▁went',\n",
       " '▁general',\n",
       " '▁show',\n",
       " '▁september',\n",
       " '4',\n",
       " '▁something',\n",
       " '▁re',\n",
       " '▁along',\n",
       " '▁men',\n",
       " '▁york',\n",
       " 'p',\n",
       " '▁m',\n",
       " '▁water',\n",
       " '▁set',\n",
       " '▁few',\n",
       " '▁october',\n",
       " '▁station',\n",
       " '▁july',\n",
       " '▁college',\n",
       " '▁old',\n",
       " '▁august',\n",
       " '▁public',\n",
       " '▁our',\n",
       " '▁black',\n",
       " '▁january',\n",
       " '▁father',\n",
       " '▁member',\n",
       " '▁band',\n",
       " '▁church',\n",
       " 'na',\n",
       " '▁co',\n",
       " '▁april',\n",
       " '▁next',\n",
       " '▁system',\n",
       " '▁got',\n",
       " '▁5',\n",
       " '▁10',\n",
       " '▁la',\n",
       " '▁b',\n",
       " '▁located',\n",
       " '▁former',\n",
       " '▁room',\n",
       " '▁song',\n",
       " '▁love',\n",
       " '▁come',\n",
       " '▁river',\n",
       " '▁east',\n",
       " '▁night',\n",
       " '▁november',\n",
       " '▁single',\n",
       " '▁party',\n",
       " '▁16',\n",
       " '▁age',\n",
       " '▁car',\n",
       " '▁population',\n",
       " '▁p',\n",
       " '▁every',\n",
       " '▁building',\n",
       " '▁st',\n",
       " '▁five',\n",
       " '▁december',\n",
       " '▁15',\n",
       " '▁body',\n",
       " '▁white',\n",
       " '▁book',\n",
       " '▁look',\n",
       " '▁though',\n",
       " '▁within',\n",
       " '▁women',\n",
       " '▁service',\n",
       " '▁without',\n",
       " '▁un',\n",
       " '▁open',\n",
       " '▁17',\n",
       " '▁large',\n",
       " '▁top',\n",
       " '▁death',\n",
       " '▁started',\n",
       " '▁great',\n",
       " '▁local',\n",
       " '▁need',\n",
       " '▁named',\n",
       " '▁should',\n",
       " '6',\n",
       " 'z',\n",
       " 'born',\n",
       " '▁once',\n",
       " '▁road',\n",
       " 'st',\n",
       " '▁moved',\n",
       " '▁born',\n",
       " '0',\n",
       " '▁british',\n",
       " '▁order',\n",
       " '▁12',\n",
       " '▁children',\n",
       " '▁built',\n",
       " '▁door',\n",
       " '▁major',\n",
       " '▁say',\n",
       " 'x',\n",
       " '▁due',\n",
       " '▁f',\n",
       " '▁village',\n",
       " '▁6',\n",
       " '▁km',\n",
       " '▁red',\n",
       " 'is',\n",
       " '▁knew',\n",
       " '▁park',\n",
       " '▁president',\n",
       " '▁main',\n",
       " '▁let',\n",
       " '▁february',\n",
       " 'f',\n",
       " '▁asked',\n",
       " '▁turned',\n",
       " 'le',\n",
       " '▁power',\n",
       " '▁art',\n",
       " '▁per',\n",
       " '▁wanted',\n",
       " '▁play',\n",
       " '▁although',\n",
       " '▁received',\n",
       " '▁different',\n",
       " '▁third',\n",
       " '▁served',\n",
       " '▁near',\n",
       " '▁himself',\n",
       " '▁final',\n",
       " '▁felt',\n",
       " '▁century',\n",
       " '▁together',\n",
       " '▁voice',\n",
       " '▁front',\n",
       " '▁based',\n",
       " '▁son',\n",
       " '▁20',\n",
       " '▁football',\n",
       " '▁times',\n",
       " '▁king',\n",
       " 'he',\n",
       " '▁behind',\n",
       " '▁died',\n",
       " '▁saw',\n",
       " '▁put',\n",
       " '▁members',\n",
       " 'ton',\n",
       " 'ers',\n",
       " '▁street',\n",
       " '▁history',\n",
       " '▁help',\n",
       " '▁mother',\n",
       " '▁award',\n",
       " '▁law',\n",
       " '▁having',\n",
       " '▁7',\n",
       " '▁point',\n",
       " '▁army',\n",
       " '▁late',\n",
       " '8',\n",
       " 'we',\n",
       " '▁center',\n",
       " '▁division',\n",
       " '▁young',\n",
       " '▁ever',\n",
       " '▁far',\n",
       " '▁across',\n",
       " '7',\n",
       " '▁games',\n",
       " '▁published',\n",
       " '▁include',\n",
       " '▁8',\n",
       " '▁told',\n",
       " '▁light',\n",
       " 'as',\n",
       " '▁hands',\n",
       " '▁18',\n",
       " '▁country',\n",
       " '▁land',\n",
       " '▁find',\n",
       " '▁often',\n",
       " '▁london',\n",
       " '▁cup',\n",
       " '▁led',\n",
       " '▁species',\n",
       " '▁why',\n",
       " '▁french',\n",
       " '▁run',\n",
       " '▁english',\n",
       " '▁j',\n",
       " '▁office',\n",
       " '▁g',\n",
       " '▁six',\n",
       " '▁must',\n",
       " '▁gave',\n",
       " '▁al',\n",
       " '▁tell',\n",
       " '▁court',\n",
       " '▁according',\n",
       " '▁among',\n",
       " '▁2010',\n",
       " 'w',\n",
       " ']',\n",
       " '▁original',\n",
       " '▁short',\n",
       " '▁full',\n",
       " '▁given',\n",
       " '▁form',\n",
       " '▁included',\n",
       " '▁million',\n",
       " '▁business',\n",
       " '▁days',\n",
       " '▁really',\n",
       " '▁enough',\n",
       " 'ar',\n",
       " '▁14',\n",
       " '▁community',\n",
       " '▁live',\n",
       " '▁council',\n",
       " '▁player',\n",
       " '9',\n",
       " '▁half',\n",
       " '▁opened',\n",
       " '▁central',\n",
       " '▁development',\n",
       " '▁san',\n",
       " '▁woman',\n",
       " 'ra',\n",
       " '▁wasn',\n",
       " '▁research',\n",
       " 'son',\n",
       " '▁lost',\n",
       " '▁might',\n",
       " 'la',\n",
       " 'el',\n",
       " '▁become',\n",
       " '▁mi',\n",
       " '▁fire',\n",
       " 'ta',\n",
       " 'do',\n",
       " '▁non',\n",
       " '▁13',\n",
       " 'ma',\n",
       " '▁seen',\n",
       " '▁2011',\n",
       " '▁close',\n",
       " '▁making',\n",
       " '▁career',\n",
       " 'de',\n",
       " '▁11',\n",
       " '▁german',\n",
       " 'ia',\n",
       " '▁always',\n",
       " '▁free',\n",
       " '▁hard',\n",
       " '▁island',\n",
       " '▁hall',\n",
       " '▁support',\n",
       " 'ne',\n",
       " '▁almost',\n",
       " '▁force',\n",
       " '▁director',\n",
       " '▁round',\n",
       " '▁9',\n",
       " '▁sure',\n",
       " '▁2012',\n",
       " '▁things',\n",
       " '▁education',\n",
       " '▁v',\n",
       " '▁married',\n",
       " '▁using',\n",
       " '▁control',\n",
       " '▁2008',\n",
       " '▁record',\n",
       " '▁inside',\n",
       " '▁students',\n",
       " '▁better',\n",
       " '▁field',\n",
       " 'up',\n",
       " '▁worked',\n",
       " '▁continued',\n",
       " 'able',\n",
       " '▁big',\n",
       " '▁title',\n",
       " '▁heart',\n",
       " '▁feel',\n",
       " '▁thing',\n",
       " '▁human',\n",
       " '▁james',\n",
       " '▁returned',\n",
       " '▁2009',\n",
       " '▁am',\n",
       " '▁give',\n",
       " 'te',\n",
       " '▁2006',\n",
       " 'da',\n",
       " '▁mind',\n",
       " '▁ii',\n",
       " 'ic',\n",
       " '▁role',\n",
       " '▁sea',\n",
       " '▁championship',\n",
       " '▁total',\n",
       " '▁act',\n",
       " '▁anything',\n",
       " '▁radio',\n",
       " '▁department',\n",
       " '▁william',\n",
       " '▁feet',\n",
       " '▁association',\n",
       " 'what',\n",
       " '▁story',\n",
       " '▁case',\n",
       " '▁military',\n",
       " '▁royal',\n",
       " '▁established',\n",
       " '▁already',\n",
       " '▁version',\n",
       " '▁2013',\n",
       " '▁various',\n",
       " 'land',\n",
       " '▁w',\n",
       " '▁period',\n",
       " '▁special',\n",
       " 'ry',\n",
       " '▁2014',\n",
       " '▁2007',\n",
       " '▁program',\n",
       " '▁nothing',\n",
       " '▁hair',\n",
       " '▁win',\n",
       " '▁rock',\n",
       " '▁does',\n",
       " 'ley',\n",
       " '▁region',\n",
       " '▁television',\n",
       " '▁living',\n",
       " '▁o',\n",
       " '▁[',\n",
       " '▁production',\n",
       " '▁working',\n",
       " '▁least',\n",
       " '▁past',\n",
       " '▁society',\n",
       " '▁written',\n",
       " '▁keep',\n",
       " '▁l',\n",
       " '▁call',\n",
       " '▁sound',\n",
       " '▁green',\n",
       " '▁england',\n",
       " '▁position',\n",
       " '▁western',\n",
       " '▁soon',\n",
       " '▁ten',\n",
       " '▁further',\n",
       " '▁others',\n",
       " '▁brother',\n",
       " 'so',\n",
       " 'ka',\n",
       " '▁taken',\n",
       " '▁dark',\n",
       " 'ie',\n",
       " 'ro',\n",
       " '▁level',\n",
       " '▁wife',\n",
       " '▁reached',\n",
       " '▁george',\n",
       " '▁ground',\n",
       " '▁union',\n",
       " '▁social',\n",
       " '▁project',\n",
       " '▁joined',\n",
       " '▁produced',\n",
       " '▁lead',\n",
       " 'ch',\n",
       " '▁course',\n",
       " '▁political',\n",
       " '▁information',\n",
       " '▁done',\n",
       " '▁post',\n",
       " '▁created',\n",
       " '▁k',\n",
       " '▁important',\n",
       " '▁appeared',\n",
       " '▁real',\n",
       " '▁david',\n",
       " '▁upon',\n",
       " '▁board',\n",
       " '▁services',\n",
       " '▁moment',\n",
       " '▁site',\n",
       " '▁2016',\n",
       " '▁2015',\n",
       " '▁battle',\n",
       " '▁works',\n",
       " '▁either',\n",
       " '▁girl',\n",
       " '▁blood',\n",
       " '▁summer',\n",
       " '▁police',\n",
       " '▁looking',\n",
       " '▁instead',\n",
       " '▁god',\n",
       " 'ist',\n",
       " '▁announced',\n",
       " '▁design',\n",
       " '▁blue',\n",
       " 'nd',\n",
       " '▁arms',\n",
       " '▁low',\n",
       " '▁followed',\n",
       " 'v',\n",
       " '▁couldn',\n",
       " '▁30',\n",
       " '▁0',\n",
       " '▁france',\n",
       " '▁words',\n",
       " 'co',\n",
       " '▁ran',\n",
       " '▁class',\n",
       " '▁outside',\n",
       " 'ness',\n",
       " 'at',\n",
       " '▁finally',\n",
       " 'um',\n",
       " '▁considered',\n",
       " '▁mouth',\n",
       " '▁space',\n",
       " '▁lake',\n",
       " 'j',\n",
       " '▁toward',\n",
       " '▁match',\n",
       " 'year',\n",
       " '▁19',\n",
       " '▁european',\n",
       " '▁present',\n",
       " '▁square',\n",
       " '▁professional',\n",
       " '▁someone',\n",
       " '▁wrote',\n",
       " '▁india',\n",
       " '▁america',\n",
       " '▁taking',\n",
       " '▁records',\n",
       " '▁northern',\n",
       " '▁dr',\n",
       " '▁described',\n",
       " '▁h',\n",
       " '▁stop',\n",
       " '▁bar',\n",
       " '▁trying',\n",
       " '▁australia',\n",
       " '▁california',\n",
       " '▁heard',\n",
       " '▁change',\n",
       " '▁miles',\n",
       " 'men',\n",
       " 'ism',\n",
       " '▁common',\n",
       " '▁paul',\n",
       " 'os',\n",
       " '▁rest',\n",
       " '▁r',\n",
       " '▁science',\n",
       " '▁video',\n",
       " '▁race',\n",
       " 'am',\n",
       " '▁gold',\n",
       " '▁leave',\n",
       " '▁yet',\n",
       " '▁wall',\n",
       " '▁hit',\n",
       " '▁25',\n",
       " '▁middle',\n",
       " '▁modern',\n",
       " '▁museum',\n",
       " '▁southern',\n",
       " '▁election',\n",
       " '▁able',\n",
       " '▁star',\n",
       " 'by',\n",
       " '▁move',\n",
       " '▁care',\n",
       " '▁pre',\n",
       " '▁daughter',\n",
       " '▁above',\n",
       " '▁founded',\n",
       " '▁today',\n",
       " '▁less',\n",
       " '▁return',\n",
       " '▁minister',\n",
       " 'me',\n",
       " '▁playing',\n",
       " '▁track',\n",
       " 'ca',\n",
       " '▁tried',\n",
       " '▁sent',\n",
       " '▁everything',\n",
       " '▁im',\n",
       " '▁met',\n",
       " '▁start',\n",
       " '▁=',\n",
       " '▁recorded',\n",
       " 'ling',\n",
       " '▁floor',\n",
       " 'don',\n",
       " '▁turn',\n",
       " 'ian',\n",
       " '▁event',\n",
       " '▁100',\n",
       " '▁brought',\n",
       " '▁seven',\n",
       " 'but',\n",
       " '▁2005',\n",
       " '▁arm',\n",
       " '▁2017',\n",
       " 'ter',\n",
       " '▁language',\n",
       " 'den',\n",
       " '▁months',\n",
       " '▁points',\n",
       " '▁mr',\n",
       " 'if',\n",
       " '▁robert',\n",
       " '▁example',\n",
       " '▁2000',\n",
       " '▁kind',\n",
       " '▁railway',\n",
       " '▁closed',\n",
       " '▁dead',\n",
       " '▁result',\n",
       " '▁release',\n",
       " '▁thomas',\n",
       " '▁michael',\n",
       " '▁word',\n",
       " 'ge',\n",
       " '▁similar',\n",
       " '▁canada',\n",
       " '▁fact',\n",
       " '▁training',\n",
       " 'go',\n",
       " '▁person',\n",
       " '▁killed',\n",
       " '▁finished',\n",
       " '▁formed',\n",
       " '▁popular',\n",
       " '▁doing',\n",
       " '▁tour',\n",
       " '▁sat',\n",
       " '▁addition',\n",
       " 'one',\n",
       " '▁child',\n",
       " '▁needed',\n",
       " '▁21',\n",
       " '▁health',\n",
       " 'well',\n",
       " '▁deep',\n",
       " 'ity',\n",
       " 'ation',\n",
       " '▁route',\n",
       " '▁table',\n",
       " '▁friend',\n",
       " '▁centre',\n",
       " '▁average',\n",
       " 'be',\n",
       " '▁rather',\n",
       " '▁lot',\n",
       " '▁current',\n",
       " '▁stood',\n",
       " '▁decided',\n",
       " '▁despite',\n",
       " 'se',\n",
       " '▁currently',\n",
       " 'ion',\n",
       " '▁bed',\n",
       " '▁festival',\n",
       " '▁money',\n",
       " '▁stage',\n",
       " '▁week',\n",
       " '▁eventually',\n",
       " '▁forces',\n",
       " '▁coming',\n",
       " '▁pulled',\n",
       " '▁idea',\n",
       " '▁boy',\n",
       " '▁mean',\n",
       " '▁developed',\n",
       " '▁brown',\n",
       " 'that',\n",
       " '▁elected',\n",
       " 'ni',\n",
       " '▁seemed',\n",
       " 'time',\n",
       " '▁throughout',\n",
       " '▁debut',\n",
       " '▁2004',\n",
       " '▁pro',\n",
       " '▁bad',\n",
       " 'rd',\n",
       " '▁says',\n",
       " '▁bay',\n",
       " '▁construction',\n",
       " '▁press',\n",
       " '▁minutes',\n",
       " '▁chief',\n",
       " '▁available',\n",
       " '▁added',\n",
       " '▁1,',\n",
       " '▁mid',\n",
       " '▁signed',\n",
       " '▁indian',\n",
       " '▁probably',\n",
       " 'ur',\n",
       " '▁originally',\n",
       " '▁maybe',\n",
       " '▁germany',\n",
       " 'my',\n",
       " '▁songs',\n",
       " '▁forward',\n",
       " '12',\n",
       " '▁cross',\n",
       " '▁grand',\n",
       " 'ko',\n",
       " '▁charles',\n",
       " 'less',\n",
       " '▁areas',\n",
       " '▁character',\n",
       " '▁di',\n",
       " '▁sir',\n",
       " 'ri',\n",
       " '▁appointed',\n",
       " '▁talk',\n",
       " '▁el',\n",
       " '▁private',\n",
       " '▁australian',\n",
       " '▁institute',\n",
       " '▁remained',\n",
       " '▁peter',\n",
       " '▁24',\n",
       " '▁possible',\n",
       " '▁sun',\n",
       " '▁committee',\n",
       " '▁media',\n",
       " '▁episode',\n",
       " '▁mark',\n",
       " '▁plan',\n",
       " '▁sold',\n",
       " '▁running',\n",
       " 'il',\n",
       " '▁hill',\n",
       " '▁study',\n",
       " '▁food',\n",
       " '▁usually',\n",
       " '▁teams',\n",
       " '▁process',\n",
       " '▁friends',\n",
       " '▁bridge',\n",
       " '▁performance',\n",
       " '▁smile',\n",
       " '▁herself',\n",
       " '▁morning',\n",
       " 'com',\n",
       " '▁1990',\n",
       " '▁else',\n",
       " '▁events',\n",
       " '▁books',\n",
       " '▁bank',\n",
       " '▁strong',\n",
       " '▁features',\n",
       " '▁list',\n",
       " 'ba',\n",
       " '▁eight',\n",
       " '▁province',\n",
       " '▁sub',\n",
       " '▁2003',\n",
       " '▁hours',\n",
       " '▁census',\n",
       " '▁quickly',\n",
       " '▁coach',\n",
       " '▁range',\n",
       " '▁whole',\n",
       " '▁hospital',\n",
       " '▁term',\n",
       " '▁network',\n",
       " '▁tv',\n",
       " '▁guitar',\n",
       " '▁believe',\n",
       " '▁performed',\n",
       " '▁lower',\n",
       " '▁seat',\n",
       " '▁official',\n",
       " '▁hear',\n",
       " '▁attack',\n",
       " '▁myself',\n",
       " '▁x',\n",
       " 'ha',\n",
       " '▁ship',\n",
       " '▁news',\n",
       " '▁beginning',\n",
       " '▁china',\n",
       " '▁shot',\n",
       " ...]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create_instances_from_document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-24T10:10:27.765935Z",
     "start_time": "2019-12-24T10:10:27.762409Z"
    }
   },
   "outputs": [],
   "source": [
    "document = tokens\n",
    "max_num_tokens = FLAGS.max_seq_length - 3\n",
    "\n",
    "target_seq_length = max_num_tokens\n",
    "if rng.random() < FLAGS.short_seq_prob:\n",
    "    target_seq_length = rng.randint(2, max_num_tokens)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-24T04:15:09.600250Z",
     "start_time": "2019-12-24T04:15:09.505082Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loading sentence piece model\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tokenization.FullTokenizer(\n",
    "      vocab_file=FLAGS.vocab_file, do_lower_case=FLAGS.do_lower_case,\n",
    "      spm_model_file=FLAGS.spm_model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-24T04:22:42.818329Z",
     "start_time": "2019-12-24T04:22:42.810931Z"
    }
   },
   "outputs": [],
   "source": [
    "input_files = []\n",
    "for input_pattern in FLAGS.input_file.split(\",\"):\n",
    "    input_files.extend(tf.gfile.Glob(input_pattern))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-24T04:22:47.925797Z",
     "start_time": "2019-12-24T04:22:47.922262Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data/sample.txt']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-24T04:25:39.973627Z",
     "start_time": "2019-12-24T04:25:39.347629Z"
    }
   },
   "outputs": [],
   "source": [
    "rng = random.Random(FLAGS.random_seed)\n",
    "instances = create_training_instances(\n",
    "  input_files, tokenizer, FLAGS.max_seq_length, FLAGS.dupe_factor,\n",
    "  FLAGS.short_seq_prob, FLAGS.masked_lm_prob, FLAGS.max_predictions_per_seq,\n",
    "  rng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-24T04:26:10.893695Z",
     "start_time": "2019-12-24T04:26:10.890154Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tokens: [CLS] ▁1 . ▁an ▁old ▁man [MASK] [MASK] [MASK] ▁village [SEP] ▁short ▁moral ▁stories ▁ - ▁an ▁old ▁man [SEP]\n",
       "segment_ids: 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1\n",
       "token_boundary: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
       "is_random_next: False\n",
       "masked_lm_positions: 6 7 8\n",
       "masked_lm_labels: ▁lived ▁in ▁the\n"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instances[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-24T04:26:37.086448Z",
     "start_time": "2019-12-24T04:26:37.083197Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tokens: [CLS] ▁the ▁10 ▁best ▁short ▁moral [MASK] [SEP] ▁some ▁of ▁these ▁stories ▁are ▁very ▁short ▁and ▁basic . ▁in ▁fact [MASK] ▁are ▁so ▁basic [MASK] ’ re ▁most ▁likely ▁featured ▁in ▁children ’ s ▁books ▁somewhere . ▁prussian [MASK] [MASK] ▁strength ▁of ▁the ▁message ▁remains ▁the [MASK] . [SEP]\n",
       "segment_ids: 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
       "token_boundary: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
       "is_random_next: False\n",
       "masked_lm_positions: 6 20 24 37 38 39 46\n",
       "masked_lm_labels: ▁stories ▁some ▁they ▁however , ▁the ▁same\n"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instances[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-24T04:38:00.102196Z",
     "start_time": "2019-12-24T04:38:00.098235Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tokens: [CLS] ▁our ▁last ▁article ▁of ▁short ▁stories ▁became ▁so [MASK] , ▁that ▁we ▁decided ▁to ▁create ▁another ▁list , ▁in [MASK] [MASK] ▁story ▁has ▁a ▁simple ▁moral ▁behind ▁it . [SEP] ▁the ▁longer ▁he ▁lived , ▁the [MASK] ▁ bile ▁he ▁was ▁becoming ▁and [MASK] [MASK] ▁poisonous ▁were ▁his ▁words . ▁people ▁avoided ▁him [MASK] ▁because [MASK] ▁misfortune ▁became ▁con tag ious . ▁it ▁was ▁even ▁unnatural ▁and ▁insulting ▁to ▁be ▁happy ▁next ▁to [MASK] [MASK] [SEP]\n",
       "segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
       "token_boundary: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
       "is_random_next: True\n",
       "masked_lm_positions: 9 20 21 37 44 45 48 54 56 57 74 75\n",
       "masked_lm_labels: ▁popular ▁which ▁every ▁more ▁the ▁more ▁his , ▁his ▁misfortune ▁him .\n"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instances[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "180.390625px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
